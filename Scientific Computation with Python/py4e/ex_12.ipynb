{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The world’s simplest web browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Mon, 11 Sep 2023 21:21:25 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"a7-54f6609245537\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 167\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving an image over HTTP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Mon, 11 Sep 2023 16:10:29 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PORT = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect((HOST, PORT)) \n",
    "mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n') #il b funziona da .encode() e trasforma il tutto in bytes\n",
    "count = 0\n",
    "picture = b\"\"  #bytes nullo\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(5120)\n",
    "    if len(data) < 1: break  #si ferma quando trova uno spazio vuoto\n",
    "    #time.sleep(0.25)\n",
    "    #count = count + len(data)\n",
    "    #print(len(data), count)\n",
    "    picture = picture + data\n",
    "\n",
    "mysock.close()\n",
    "\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = picture.find(b\"\\r\\n\\r\\n\")\n",
    "print('Header length', pos)\n",
    "print(picture[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "picture = picture[pos+4:] #tolgo i 4 caratteri \\r\\n\\r\\n \n",
    "fhand = open(\"stuff.jpg\", \"wb\") #write in bynary \n",
    "fhand.write(picture)\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urljpeg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving web pages with urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().rstrip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading binary files using urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230210"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg').read()\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "fhand.write(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230210 characters copied.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg')\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "size = 0\n",
    "while True: \n",
    "    info = img.read(100000)  #you onlu read 100,000 characters at a time and then write those characters to the cover3.jpg file before retrieving the next 100,000 characters of data from the web.\n",
    "    if len(info) < 1: break\n",
    "    size = size + len(info)\n",
    "    fhand.write(info)\n",
    "\n",
    "print(size, 'characters copied.')\n",
    "fhand.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML using regular expressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to parse HTML is to use regular expressions to repeatedly search for and extract substrings that match a particular pattern.\n",
    "\n",
    "Here a simpe web page:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1875027082.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    <h1>The First Page</h1>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<h1>The First Page</h1>\n",
    "<p>\n",
    "If you like, you can switch to the\n",
    "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
    "Second Page </a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a well-formed regular expression to match and extract the link values from the above text as follows:\n",
    "\n",
    "regexp = 'href=\"http[s]?://.+?\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.python.org/3/index.html\n",
      "https://www.python.org/\n",
      "https://docs.python.org/3.13/\n",
      "https://docs.python.org/3.12/\n",
      "https://docs.python.org/3.11/\n",
      "https://docs.python.org/3.10/\n",
      "https://docs.python.org/3.9/\n",
      "https://docs.python.org/3.8/\n",
      "https://docs.python.org/3.7/\n",
      "https://docs.python.org/3.6/\n",
      "https://docs.python.org/3.5/\n",
      "https://docs.python.org/2.7/\n",
      "https://www.python.org/doc/versions/\n",
      "https://www.python.org/dev/peps/\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://www.python.org/doc/av/\n",
      "https://devguide.python.org/\n",
      "https://www.python.org/\n",
      "https://devguide.python.org/docquality/#helping-with-documentation\n",
      "https://docs.python.org/3.13/\n",
      "https://docs.python.org/3.12/\n",
      "https://docs.python.org/3.11/\n",
      "https://docs.python.org/3.10/\n",
      "https://docs.python.org/3.9/\n",
      "https://docs.python.org/3.8/\n",
      "https://docs.python.org/3.7/\n",
      "https://docs.python.org/3.6/\n",
      "https://docs.python.org/3.5/\n",
      "https://docs.python.org/2.7/\n",
      "https://www.python.org/doc/versions/\n",
      "https://www.python.org/dev/peps/\n",
      "https://wiki.python.org/moin/BeginnersGuide\n",
      "https://wiki.python.org/moin/PythonBooks\n",
      "https://www.python.org/doc/av/\n",
      "https://devguide.python.org/\n",
      "https://www.python.org/\n",
      "https://www.python.org/psf/donations/\n",
      "https://www.sphinx-doc.org/\n"
     ]
    }
   ],
   "source": [
    "# Search for link values within URL input\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl  #per accedere online? \n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "links = re.findall(b'href=\"(http[s]?://.+?)\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML using BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG: <a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>\n",
      "URL http://www.dr-chuck.com/page2.htm\n",
      "CONTENTS ['\\nSecond Page']\n",
      "ATT {'href': 'http://www.dr-chuck.com/page2.htm'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')  #è un dizionario circa\n",
    "for tag in tags:\n",
    "    print('TAG:', tag),\n",
    "    print('URL', tag.get('href', None)) \n",
    "    print('CONTENTS', tag.contents)\n",
    "    print('ATT', tag.attrs)\n",
    "    \n",
    "    \n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "\n",
    " Change the socket program socket1.py to prompt the user for the URL so it can read any web page. You can use split('/') to break the URL into its component parts so you can extract the host name for the socket connect call. Add error checking using try and except to handle the condition where the user enters an improperly formatted or non-existent URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Tue, 12 Sep 2023 11:44:59 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "url = input('Enter url - ')\n",
    "\n",
    "url_prefix = url.split('/')  # if the url starts with http(s)    \n",
    "if len(url_prefix)> 1 and url_prefix[0] == re.findall('http[s]?:', url)[0]:\n",
    "    host = re.findall('http[s]?://(.+\\..+?)/.+', url)[0]\n",
    "\n",
    "else: # if it starts with wwww.something\n",
    "    host = url\n",
    "\n",
    "\n",
    "# port number: 80 for HTTP\n",
    "portnum = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# testing the url's validity\n",
    "try:\n",
    "    mysock.connect((host, portnum))\n",
    "    # 'cmd' is edited to incorporate user input\n",
    "    cmd = f'GET {url} HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "    mysock.send(cmd)\n",
    "\n",
    "    while True:\n",
    "        data = mysock.recv(512)\n",
    "        if len(data) < 1:\n",
    "            break\n",
    "            \n",
    "        print(data.decode(),end='')\n",
    "\n",
    "    mysock.close()\n",
    "\n",
    "except:\n",
    "    print(\"Please enter an existing URL!\")\n",
    "    exit()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12.2\n",
    "\n",
    "Change your socket program so that it counts the number of characters it has received and stops displaying any text after it has shown 3000 characters. The program should retrieve the entire document and count the total number of characters and display the count of the number of characters at the end of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Tue, 12 Sep 2023 10:50:09 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"22a0-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 8864\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "Romeo and Juliet\n",
      "Act 2, Scene 2 \n",
      "\n",
      "SCENE II. Capulet's orchard.\n",
      "\n",
      "Enter ROMEO\n",
      "\n",
      "ROMEO\n",
      "\n",
      "He jests at scars that never felt a wound.\n",
      "JULIET appears above at a window\n",
      "\n",
      "But, soft! what light through yonder window breaks?\n",
      "It is the east, and Juliet is the sun.\n",
      "Arise, fair sun, and kill the envious moon,\n",
      "Who is already sick and pale with grief,\n",
      "That thou her maid art far more fair than she:\n",
      "Be not her maid, since she is envious;\n",
      "Her vestal livery is but sick and green\n",
      "And none but fools do wear it; cast it off.\n",
      "It is my lady, O, it is my love!\n",
      "O, that she knew she were!\n",
      "She speaks yet she says nothing: what of that?\n",
      "Her eye discourses; I will answer it.\n",
      "I am too bold, 'tis not to me she speaks:\n",
      "Two of the fairest stars in all the heaven,\n",
      "Having some business, do entreat her eyes\n",
      "To twinkle in their spheres till they return.\n",
      "What if her eyes were there, they in her head?\n",
      "The brightness of her cheek would shame those stars,\n",
      "As daylight doth a lamp; her eyes in heaven\n",
      "Would through the airy region stream so bright\n",
      "That birds would sing and think it were not night.\n",
      "See, how she leans her cheek upon her hand!\n",
      "O, that I were a glove upon that hand,\n",
      "That I might touch that cheek!\n",
      "\n",
      "JULIET\n",
      "\n",
      "Ay me!\n",
      "\n",
      "ROMEO\n",
      "\n",
      "She speaks:\n",
      "O, speak again, bright angel! for thou art\n",
      "As glorious to this night, being o'er my head\n",
      "As is a winged messenger of heaven\n",
      "Unto the white-upturned wondering eyes\n",
      "Of mortals that fall back to gaze on him\n",
      "When he bestrides the lazy-pacing clouds\n",
      "And sails upon the bosom of the air.\n",
      "\n",
      "JULIET\n",
      "\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy father and refuse thy name;\n",
      "Or, if thou wilt not, be but sworn my love,\n",
      "And I'll no longer be a Capulet.\n",
      "\n",
      "ROMEO\n",
      "\n",
      "[Aside] Shall I hear more, or shall I speak at this?\n",
      "\n",
      "JULIET\n",
      "\n",
      "'Tis but thy name that is my enemy;\n",
      "Thou art thyself, though not a Montague.\n",
      "What's Montague? it is nor hand, nor foot,\n",
      "Nor arm, nor face, nor any other part\n",
      "Belonging to a man. O, be some other name!\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n",
      "So Romeo would, were he not Romeo call'd,\n",
      "Retain that dear perfection which he owes\n",
      "Without that title. Romeo, doff thy name,\n",
      "And for that name which is no part of thee\n",
      "Take all myself.\n",
      "\n",
      "ROMEO\n",
      "\n",
      "I take thee at thy word:\n",
      "Call me but love, and I'll be new baptized;\n",
      "Henceforth I never will be Romeo.\n",
      "\n",
      "JULIET\n",
      "\n",
      "What man art thou that thus bescreen'd in night\n",
      "So stumblest on my counsel?\n",
      "\n",
      "ROMEO\n",
      "\n",
      "By a name\n",
      "I know not how to tell thee who I am:\n",
      "My name, dear saint, is hateful to myself,\n",
      "Because it is an enemy to thee;\n",
      "Had I it written, I would tear the word.\n",
      "\n",
      "JULIET\n",
      "\n",
      "My ears have not yet drunk a hundred words\n",
      "Of\n",
      "\n",
      "Display stopped at character count of 3000\n",
      "Total number of characters received: 9236\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "url = input('Enter url - ')\n",
    "\n",
    "url_prefix = url.split('/')  # if the url starts with http(s)    \n",
    "if len(url_prefix)> 1 and url_prefix[0] == re.findall('http[s]?:', url)[0]:\n",
    "    host = re.findall('http[s]?://(.+\\..+?)/.+', url)[0]\n",
    "\n",
    "else: # if it starts with wwww.something\n",
    "    host = url\n",
    "\n",
    "\n",
    "# port number: 80 for HTTP\n",
    "portnum = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "#new variables for the assignment \n",
    "displayCount = 0 #conta i caratteri che sono sul display, alla fine deve essere uguale charLimit\n",
    "totalCount = 0 #conta tutti i caratteri del file\n",
    "charLimit =3000\n",
    "\n",
    "\n",
    "# testing the url's validity\n",
    "try:\n",
    "    mysock.connect((host, portnum))\n",
    "    # 'cmd' is edited to incorporate user input\n",
    "    cmd = f'GET {url} HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "    mysock.send(cmd)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        data = mysock.recv(500)\n",
    "        totalCount +=len(data) \n",
    "        \n",
    "        if len(data) < 1:\n",
    "            break\n",
    "        elif totalCount > charLimit:\n",
    "            displayCount = charLimit\n",
    "            continue\n",
    "            \n",
    "        print(data.decode()[:charLimit],end='')\n",
    "    \n",
    "    # output as per requested\n",
    "    print(f\"\\n\\nDisplay stopped at character count of {displayCount}\")\n",
    "    print(f\"Total number of characters received: {totalCount}\")\n",
    "    \n",
    "except:\n",
    "    print(\"Please enter an existing URL!\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Use urllib to replicate the previous exercise of (1) retrieving the document from a URL, (2) displaying up to 3000 characters, and (3) counting the overall number of characters in the document. Don’t worry about the headers for this exercise, simply show the first 3000 characters of the document contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n",
      "\n",
      "Total characters in the document: 467\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "\n",
    "url = input('Enter url - ')\n",
    "\n",
    "#file handle with the url \n",
    "fhand = urllib.request.urlopen(url)\n",
    "content = fhand.read() \n",
    "content = content.decode()\n",
    "\n",
    "charLimit = 3000\n",
    "\n",
    "print(content[:charLimit])\n",
    "print('Total characters in the document:', len(content))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12.4\n",
    "Change the urllinks.py program to extract and count paragraph (p) tags from the retrieved HTML document and display the count of the paragraphs as the output of your program. Do not display the paragraph text, only count them. Test your program on several small web pages as well as some larger web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paragraphs 7\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('p')\n",
    "count = len(tags)\n",
    "   \n",
    "print('Total number of paragraphs', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12.5\n",
    "\n",
    "(Advanced) Change the socket program so that it only shows data after the headers and a blank line have been received. Remember that recv receives characters (newlines and all), not lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import re\n",
    "\n",
    "url = input('Enter url - ')\n",
    "\n",
    "url_prefix = url.split('/')  # if the url starts with http(s)    \n",
    "if len(url_prefix)> 1 and url_prefix[0] == re.findall('http[s]?:', url)[0]:\n",
    "    host = re.findall('http[s]?://(.+\\..+?)/.+', url)[0]\n",
    "\n",
    "else: # if it starts with wwww.something\n",
    "    host = url\n",
    "\n",
    "\n",
    "# port number: 80 for HTTP\n",
    "portnum = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# testing the url's validity\n",
    "mysock.connect((host, portnum))\n",
    "    # 'cmd' is edited to incorporate user input\n",
    "cmd = f'GET {url} HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "text = b''\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "            break\n",
    "    text +=data \n",
    "header_pos = text.find(b'\\r\\n\\r\\n')\n",
    "content = text[header_pos+4:].decode()\n",
    "\n",
    "print(content)\n",
    "    \n",
    "mysock.close()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12.6\n",
    "\n",
    "Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "Actual data: http://py4e-data.dr-chuck.net/comments_1881372.html (Sum ends with 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum 2531\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "countSum = 0\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    countSum += int(tag.contents[0])\n",
    "print( 'Sum', countSum)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12.7\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html \n",
    "Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "\n",
    "Sequence of names: Fikret Montgomery Mhairade Butchi Anayah \n",
    "\n",
    "Last name in sequence: Anayah\n",
    "\n",
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Parmin.html \n",
    "\n",
    "Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "\n",
    "**_Hint_**: The first character of the name of the last page that you will load is: R\n",
    "\n",
    "_Strategy_\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\n",
    "\n",
    "**Sample execution**\n",
    "\n",
    "Here is a sample execution of a solution:\n",
    "```\n",
    "$ python3 solution.py\n",
    "Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Enter count: 4\n",
    "Enter position: 3\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n",
    "``` \n",
    "The answer to the assignment for this execution is \"Anayah\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Parmin.html \n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Ali.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Aimeeleigh.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Maya.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Naima.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Caidie.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Rahma.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Rihanna.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "#chiedere l'url \n",
    "url = input('Enter the url - ')\n",
    "#chiedo quante volte devo ripetere il loop - repeatCount- e da quale posiione partire - startPosition. \n",
    "repeatCount = int(input('Enter count:'))\n",
    "startPosition = int(input('Enter position:'))\n",
    "\n",
    "i = 0 \n",
    "while i <= repeatCount:\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    print('Retrieving:', url) \n",
    "    \n",
    "    #cerco il tag anchor (a)\n",
    "    tags = soup('a') #lista con <a href =link> contenuto </a>\n",
    "    #avrò una serie di tag, mi interessa il tag[startPosition-1] il link sarà quindi tag[startPosition - 1].get('href')\n",
    "    #rendo il link nuovo il nuovo url e itero il processo\n",
    "    url = tags[startPosition - 1].get('href')\n",
    "    i += 1 \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
